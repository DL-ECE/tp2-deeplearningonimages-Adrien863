{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "convolution.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "R2JALA0rnGE1",
        "q7Ucg7xRQDwn"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DL-ECE/tp2-deeplearningonimages-Adrien863/blob/master/convolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeT-9bRdonUj"
      },
      "source": [
        "# TP-2 Deep Learning on Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll9cmbS4QIjW"
      },
      "source": [
        "## Clothes images classification using Fashion-MNIST dataset\n",
        "\n",
        "In this notebook you will train your second and even third neural network. \n",
        "\n",
        "Feel free to look back at the Lecture-2 slides to complete the cells below.\n",
        "\n",
        "\n",
        "\n",
        "All the dependencies are installed. Below we import them and will be using them in all our notebooks.\n",
        "Please feel free to look arround and look at their API.\n",
        "The student should be limited to these imports to complete this work.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8HwsnlpWABC"
      },
      "source": [
        "# Import the different module we will need in this notebook\n",
        "import os\n",
        "\n",
        "# To read and compute on Images: imageio [imageio doc](https://imageio.readthedocs.io/en/stable/)\n",
        "# To create some plot and figures: matplolib [matplotlib doc](https://matplotlib.org/)\n",
        "# To do computation on matrix and vectors: numpy [numpy doc](https://numpy.org/)\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# To do computation on matrix and vectors and automatic differenciation: pytorch [torch doc](https://pytorch.org/docs/stable/index.html)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# To do some computation on images with pytorch direclty on the GPU [torchvision doc](https://pytorch.org/vision)\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST, FashionMNIST\n",
        "import random\n",
        "import tqdm.notebook as tq\n",
        "\n",
        "# To get the same data as TP1 \n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "# enable tpu computation\n",
        "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6hnJJcPSJcu"
      },
      "source": [
        "# In order to have some reproducable results and easier debugging \n",
        "# we fix the seed of random.\n",
        "random.seed(1342)\n",
        "np.random.seed(1342)\n",
        "torch.manual_seed(1342)\n",
        "torch.cuda.manual_seed_all(1342)\n",
        "\n",
        "import builtins as __builtin__\n",
        "def print(*args, **kwargs):\n",
        "    \"\"\"My custom print() function.\"\"\"\n",
        "    return __builtin__.print(*args, **kwargs, end='\\n\\n')"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2JALA0rnGE1"
      },
      "source": [
        "## Refresh on numpy and images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_6UGR8EUgqi",
        "outputId": "3a7c523f-50d8-48fe-9655-482c67a13e6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Let's do again basics of numpy \n",
        "mat_numpy = np.arange(15).reshape(3, 5)\n",
        "print(mat_numpy) # Create a vector from 0 to 14 and reshape it into a Matrix 3X5\n",
        "\n",
        "print(mat_numpy.shape) # Return the size of the matrix (3, 5)\n",
        "\n",
        "print(mat_numpy[0]) # Return the first row of the matrix \n",
        "\n",
        "print(mat_numpy[0,3]) # Return first row and 4th column  element \n",
        "\n",
        "# Also interesting with higher dimension \n",
        "# Below can be though of 2 3X4 matrix \n",
        "tensor = np.zeros((2,3,4))   # Create an tensor of shape [2,2,2] of all zeros\n",
        "print(tensor)                # Prints [[[0. 0. 0. 0.]\n",
        "                             #          [0. 0. 0. 0.]\n",
        "                             #          [0. 0. 0. 0.]]\n",
        "                             #        [[0. 0. 0. 0.]\n",
        "                             #         [0. 0. 0. 0.]\n",
        "                             #         [0. 0. 0. 0.]]]"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  2  3  4]\n",
            " [ 5  6  7  8  9]\n",
            " [10 11 12 13 14]]\n",
            "\n",
            "(3, 5)\n",
            "\n",
            "[0 1 2 3 4]\n",
            "\n",
            "3\n",
            "\n",
            "[[[0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHj2OBNMw5VA"
      },
      "source": [
        "Now it's your turn create a function that return a tensor of shape \n",
        "n_rowsxn_columsxn_channels that contains a default value every where"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR0fNMzPwtem"
      },
      "source": [
        "def build_image_like_tensor(n_rows:int, n_colums: int, n_channels:int, default_value: int)-> np.ndarray:\n",
        "  \"\"\"Create a tensor of 3 dimension. \n",
        "     It should have a shape similar to (n_rows, n_colums, n_channels)\n",
        "     It should be containing the default value set by default_value\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "  return np.ones((n_rows,n_colums,n_channels),dtype=np.int64)*default_value"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYPMAOSdxi8S"
      },
      "source": [
        "# Create 3 different tensors with the above function containing different value between [0,255]\n",
        "# Uncomment the 3 line below and complete with your answer \n",
        "\n",
        "white_like = build_image_like_tensor(1,1,3,255)\n",
        "gray_like = build_image_like_tensor(1,1,3,128)\n",
        "black_like = build_image_like_tensor(1,1,3,0)\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI1sR5lWyTvG"
      },
      "source": [
        "# Each of the tensor that you have created can be seen as an image. Use here is the way to display it using matplotlib imshow:\n",
        "def plot_one_tensor(image_tensor: np.array):\n",
        "    \"\"\"Function to plot the image tensor\"\"\"\n",
        "    plt.imshow(image_tensor, cmap='gray')"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMKHtF-FnGFP",
        "outputId": "530d1f5e-d016-4f46-d193-3192821fd6e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plot_one_tensor(white_like)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQcAAAD4CAYAAADhGCPfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAL9klEQVR4nO3dX4id9Z3H8fdHQ7oXauufEEUT49LcxFJsewjbi64LTSHuRSJ0d6usNIKQC1fo0u1FIOCF3qilfy4UdoO7kPXGWmFpwBSr2UpvqtuRuoItmlS2GBtN2i1CEetKv3uRx91x+M5M6vk3Sd4vGOZ5zvPj/L45Ou85czLkpKqQpKUumPcAktYm4yCpZRwktYyDpJZxkNRaN+8BlnPFFVfUli1b5j2GdE57/vnnf11VG7prazYOW7ZsYWFhYd5jSOe0JL9c7po/VkhqGQdJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGoZB0kt4yCpZRwktYyDpJZxkNQyDpJaxkFSyzhIao0VhySXJXkqydHh86UrrL0kyfEkD46zp6TZGPeZwz7gSFVtBY4M58u5F/jRmPtJmpFx47AbODgcHwRu7hYl+QywEfjBmPtJmpFx47Cxqk4Mx29wOgAfkOQC4BvA11a7syR7kywkWTh16tSYo0kax6r/+nSSp4Erm0v7F59UVSXp3pX3TuBwVR1PsuJeVXUAOAAwGo18h19pjlaNQ1XtWO5akjeTXFVVJ5JcBZxsln0W+FySO4GLgPVJfldVK70+IWnOxn3fikPAHuC+4fP3li6oqr99/zjJ7cDIMEhr37ivOdwHfCHJUWDHcE6SUZKHxx1O0vykam3+aD8ajcp3vJKmK8nzVTXqrvkbkpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1DIOklrGQVLLOEhqGQdJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGoZB0kt4yCpZRwktYyDpJZxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1DIOklrGQVJrrDgkuSzJU0mODp8vbdbckOTHSV5K8mKSL42zp6TZGPeZwz7gSFVtBY4M50u9DXy5qq4HdgLfTvKxMfeVNGXjxmE3cHA4PgjcvHRBVb1SVUeH418BJ4ENY+4racrGjcPGqjoxHL8BbFxpcZLtwHrgF2PuK2nK1q22IMnTwJXNpf2LT6qqktQK93MV8Aiwp6r+sMyavcBegM2bN682mqQpWjUOVbVjuWtJ3kxyVVWdGL74Ty6z7hLgCWB/VT27wl4HgAMAo9Fo2dBImr5xf6w4BOwZjvcA31u6IMl64N+Af62qx8fcT9KMjBuH+4AvJDkK7BjOSTJK8vCw5m+APwduT/LC8HHDmPtKmrJUrc1n76PRqBYWFuY9hnROS/J8VY26a/6GpKSWcZDUMg6SWsZBUss4SGoZB0kt4yCpZRwktYyDpJZxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1DIOklrGQVLLOEhqGQdJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGoZB0kt4yCpZRwktYyDpJZxkNQyDpJaE4lDkp1JXk5yLMm+5vpHknxnuP5cki2T2FfS9IwdhyQXAg8BNwHbgFuTbFuy7A7gt1X1ceBbwP3j7itpuibxzGE7cKyqXq2qd4FHgd1L1uwGDg7HjwOfT5IJ7C1pSiYRh6uB1xadHx9ua9dU1XvAW8DlS+8oyd4kC0kWTp06NYHRJH1Ya+oFyao6UFWjqhpt2LBh3uNI57VJxOF1YNOi82uG29o1SdYBHwV+M4G9JU3JJOLwE2BrkuuSrAduAQ4tWXMI2DMc/xXw71VVE9hb0pSsG/cOquq9JHcBTwIXAv9SVS8luQdYqKpDwD8DjyQ5Bvw3pwMiaQ0bOw4AVXUYOLzktrsXHb8D/PUk9pI0G2vqBUlJa4dxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1DIOklrGQVLLOEhqGQdJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGoZB0kt4yCpZRwktYyDpJZxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1JpIHJLsTPJykmNJ9jXXv5rkZ0leTHIkybWT2FfS9IwdhyQXAg8BNwHbgFuTbFuy7KfAqKo+CTwOPDDuvpKmaxLPHLYDx6rq1ap6F3gU2L14QVX9sKreHk6fBa6ZwL6SpmgScbgaeG3R+fHhtuXcAXx/AvtKmqJ1s9wsyW3ACLhxmet7gb0AmzdvnuFkkpaaxDOH14FNi86vGW77gCQ7gP3Arqr6fXdHVXWgqkZVNdqwYcMERpP0YU0iDj8Btia5Lsl64Bbg0OIFST4F/BOnw3ByAntKmrKx41BV7wF3AU8CPwceq6qXktyTZNew7OvARcB3k7yQ5NAydydpjZjIaw5VdRg4vOS2uxcd75jEPpJmx9+QlNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1DIOklrGQVLLOEhqGQdJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGoZB0kt4yCpZRwktYyDpJZxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1DIOklrGQVJrInFIsjPJy0mOJdm3wrovJqkko0nsK2l6xo5DkguBh4CbgG3ArUm2NesuBr4CPDfunpKmbxLPHLYDx6rq1ap6F3gU2N2suxe4H3hnAntKmrJJxOFq4LVF58eH2/5Pkk8Dm6rqiZXuKMneJAtJFk6dOjWB0SR9WFN/QTLJBcA3gX9YbW1VHaiqUVWNNmzYMO3RJK1gEnF4Hdi06Pya4bb3XQx8AngmyX8BfwYc8kVJaW2bRBx+AmxNcl2S9cAtwKH3L1bVW1V1RVVtqaotwLPArqpamMDekqZk7DhU1XvAXcCTwM+Bx6rqpST3JNk17v1Lmo91k7iTqjoMHF5y293LrP2LSewpabr8DUlJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGoZB0kt4yCpZRwktYyDpJZxkNQyDpJaxkFSyzhIaqWq5j1DK8kp4JdTuOsrgF9P4X6n5Wya92yaFc6ueac167VV1f5rzms2DtOSZKGqzpp/3PZsmvdsmhXOrnnnMas/VkhqGQdJrfMxDgfmPcAf6Wya92yaFc6ueWc+63n3moOkM3M+PnOQdAaMg6TWOR+HJJcleSrJ0eHzpSusvSTJ8SQPznLGJTOsOm+SG5L8OMlLSV5M8qUZz7gzyctJjiXZ11z/SJLvDNefS7JllvMtmWW1Wb+a5GfD43gkybXzmHPRPCvOu2jdF5PUNN9z9pyPA7APOFJVW4Ejw/ly7gV+NJOplncm874NfLmqrgd2At9O8rFZDJfkQuAh4CZgG3Brkm1Llt0B/LaqPg58C7h/FrMtdYaz/hQYVdUngceBB2Y75f87w3lJcjHwFeC5ac5zPsRhN3BwOD4I3NwtSvIZYCPwgxnNtZxV562qV6rq6HD8K+Ak0P6W2xRsB45V1atV9S7wKKdnXmzxn+Fx4PNJMqP5Flt11qr6YVW9PZw+y+l3iZ+XM3ls4fQ3sfuBd6Y5zPkQh41VdWI4foPTAfiAJBcA3wC+NsvBlrHqvIsl2Q6sB34x7cEGVwOvLTo/PtzWrhneaPkt4PKZTLfMHINu1sXuAL4/1YlWtuq8ST4NbKqqJ6Y9zETeSHfekjwNXNlc2r/4pKoqSfd3t3cCh6vq+Cy+wU1g3vfv5yrgEWBPVf1hslOeX5LcBoyAG+c9y3KGb2LfBG6fxX7nRByqasdy15K8meSqqjoxfDGdbJZ9FvhckjuBi4D1SX5XVSu9PjHPeUlyCfAEsL+qnp3GnMt4Hdi06Pya4bZuzfEk64CPAr+ZzXjtHO/rZiXJDk6H+caq+v2MZuusNu/FwCeAZ4ZvYlcCh5LsqqqFiU9TVef0B/B1YN9wvA94YJX1twMPruV5Of1jxBHg7+cw3zrgVeC6YY7/BK5fsubvgH8cjm8BHpvTY3kms36K0z+SbZ3Xf/M/Zt4l65/h9Iup05ln3g/IDB7wy4cvpKPA08Blw+0j4OFm/bzjsOq8wG3A/wAvLPq4YYYz/iXwyvBFtX+47R5g13D8J8B3gWPAfwB/OsfHc7VZnwbeXPQ4Hprz/68rzrtk7VTj4K9PS2qdD39bIelDMA6SWsZBUss4SGoZB0kt4yCpZRwktf4X3vkOmMozwkwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8nnjpziXJyo",
        "outputId": "6de1355b-e3b3-4bc6-c008-e47c1220453a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plot_one_tensor(gray_like)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ6-IuvlnGFW",
        "outputId": "386d1ec5-f19c-4215-b37d-e2d5920eb456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plot_one_tensor(black_like)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncTl3AjcnGFb"
      },
      "source": [
        "We saw that an digital image is the combination of a 3 channel tensor RGB. \n",
        "Each channel represent respectively the R red componant, G greed componant, B blue componant. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUtptCwanGFc"
      },
      "source": [
        "# Create again 3 image tensors with your function\n",
        "# Then change them to be representing a red, a green, a blue image\n",
        "# Uncomment the 3 line below and complete with your answer \n",
        "\n",
        "\n",
        "red_like = build_image_like_tensor(1,1,3,0)\n",
        "red_like[:,:,0] = 255\n",
        "green_like = build_image_like_tensor(1,1,3,0)\n",
        "green_like[:,:,1] = 255\n",
        "blue_like = build_image_like_tensor(1,1,3,0)\n",
        "blue_like[:,:,2] = 255\n",
        " "
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jXLgMmfnGFh",
        "outputId": "3ca194b3-5862-4460-b65c-bfc4e25c67ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plot_one_tensor(red_like)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQcAAAD4CAYAAADhGCPfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAALx0lEQVR4nO3dX4id9Z3H8fdHQ7oXaus/YtDUuDQ3sRTbHsL2outFU4h7kQhdtspKIwi5cIUu3V4Ecqc3aumfC4Xd4C5kvbFWWDpgi9VspTfVdUKtYIsmlS3GRmO7RSjSutLvXszj7jj7nZnU82+SvF8Q5nnO8+P8vhmd95xzMsxJVSFJK10w7wEkbUzGQVLLOEhqGQdJLeMgqbVp3gOs5oqkts97COkcdwx+XVVXdtc2bBy2A4vzHkI6xwV+udo1n1ZIahkHSS3jIKllHCS1jIOklnGQ1DIOklrGQVLLOEhqGQdJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGqNFYcklyV5Msnx4eOla6y9JMnJJA+Ms6ek2Rj3kcNB4GhV7QCODueruQf40Zj7SZqRceOwDzgyHB8Bbu4WJfk0sAX4wZj7SZqRceOwpapODcevsxSA90lyAfB14Kvr3VmSA0kWkyy+OeZgksaz7m+fTvIUcFVz6dDyk6qqJN278t4JfK+qTiZZc6+qOgwcBhj19yVpRtaNQ1XtXu1akjeSbK2qU0m2AqebZZ8BPpvkTuAiYHOS31XVWq9PSJqzcd+3YgHYD9w7fPzuygVV9bfvHSe5HRgZBmnjG/c1h3uBzyc5DuwezkkySvLQuMNJmp9Ubcyn9qOkfMcraboCx6pq1F3zJyQltYyDpJZxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1DIOklrGQVLLOEhqGQdJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGoZB0kt4yCpZRwktYyDpJZxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOk1lhxSHJZkieTHB8+XtqsuSHJj5O8mOSFJF8cZ09JszHuI4eDwNGq2gEcHc5Xehv4UlVdD+wBvpXkI2PuK2nKxo3DPuDIcHwEuHnlgqp6uaqOD8e/Ak4DV465r6QpGzcOW6rq1HD8OrBlrcVJdgGbgV+Mua+kKdu03oIkTwFXNZcOLT+pqkpSa9zPVuBhYH9V/XGVNQeAAwAfXW8wSVO1bhyqavdq15K8kWRrVZ0avvhPr7LuEuBx4FBVPbPGXoeBwwCjNUIjafrGfVqxAOwfjvcD3125IMlm4N+Af62qx8bcT9KMjBuHe4HPJzkO7B7OSTJK8tCw5m+AvwRuT/L88OeGMfeVNGWp2piP3kdJLc57COkcFzhWVaPumj8hKallHCS1jIOklnGQ1DIOklrGQVLLOEhqGQdJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGoZB0kt4yCpZRwktYyDpJZxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1DIOklrGQVLLOEhqGQdJLeMgqWUcJLWMg6TWROKQZE+Sl5KcSHKwuf6hJN8erj+bZPsk9pU0PWPHIcmFwIPATcBO4NYkO1csuwP4bVV9DPgmcN+4+0qarkk8ctgFnKiqV6rqHeARYN+KNfuAI8PxY8DnkmQCe0uakknE4Wrg1WXnJ4fb2jVV9S7wFnD5yjtKciDJYpLFNycwmKQPbkO9IFlVh6tqVFWjK+c9jHSem0QcXgO2LTu/ZritXZNkE/Bh4DcT2FvSlEwiDs8BO5Jcl2QzcAuwsGLNArB/OP5r4N+rqiawt6Qp2TTuHVTVu0nuAp4ALgT+papeTHI3sFhVC8A/Aw8nOQH8F0sBkbSBZaN+Ax8ltTjvIaRzXOBYVY26axvqBUlJG4dxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1DIOklrGQVLLOEhqGQdJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGoZB0kt4yCpZRwktYyDpJZxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1JpIHJLsSfJSkhNJDjbXv5LkZ0leSHI0ybWT2FfS9IwdhyQXAg8CNwE7gVuT7Fyx7CfAqKo+ATwG3D/uvpKmaxKPHHYBJ6rqlap6B3gE2Ld8QVX9sKreHk6fAa6ZwL6SpmgScbgaeHXZ+cnhttXcAXx/AvtKmqJNs9wsyW3ACLhxlesHgAMAH53hXJL+v0k8cngN2Lbs/JrhtvdJshs4BOytqj90d1RVh6tqVFWjKycwmKQPbhJxeA7YkeS6JJuBW4CF5QuSfBL4J5bCcHoCe0qasrHjUFXvAncBTwA/Bx6tqheT3J1k77Dsa8BFwHeSPJ9kYZW7k7RBpKrmPUNrlNTivIeQznGBY1U16q75E5KSWsZBUss4SGoZB0kt4yCpZRwktYyDpJZxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1DIOklrGQVLLOEhqGQdJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGoZB0kt4yCpZRwktYyDpJZxkNQyDpJaxkFSyzhIak0kDkn2JHkpyYkkB9dY94UklWQ0iX0lTc/YcUhyIfAgcBOwE7g1yc5m3cXAl4Fnx91T0vRN4pHDLuBEVb1SVe8AjwD7mnX3APcBv5/AnpKmbBJxuBp4ddn5yeG2/5XkU8C2qnp8rTtKciDJYpLFNycwmKQPbuovSCa5APgG8A/rra2qw1U1qqrRldMeTNKaJhGH14Bty86vGW57z8XAx4Gnk/wn8BfAgi9KShvbJOLwHLAjyXVJNgO3AAvvXayqt6rqiqraXlXbgWeAvVW1OIG9JU3J2HGoqneBu4AngJ8Dj1bVi0nuTrJ33PuXNB+pqnnP0BolPrSQpixwrKrap/j+hKSklnGQ1DIOklrGQVLLOEhqGQdJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGoZB0kt4yCpZRwktTbsL3tJ8ibwyync9RXAr6dwv9NyNs17Ns0KZ9e805r12qpqf5/zho3DtCRZXO0332xEZ9O8Z9OscHbNO49ZfVohqWUcJLXOxzgcnvcAf6Kzad6zaVY4u+ad+azn3WsOks7M+fjIQdIZMA6SWud8HJJcluTJJMeHj5eusfaSJCeTPDDLGVfMsO68SW5I8uMkLyZ5IckXZzzjniQvJTmR5GBz/UNJvj1cfzbJ9lnOt2KW9Wb9SpKfDZ/Ho0muncecy+ZZc95l676QpKb5nrPnfByAg8DRqtoBHB3OV3MP8KOZTLW6M5n3beBLVXU9sAf4VpKPzGK4JBcCDwI3ATuBW5PsXLHsDuC3VfUx4JvAfbOYbaUznPUnwKiqPgE8Btw/2yn/zxnOS5KLgS8Dz05znvMhDvuAI8PxEeDmblGSTwNbgB/MaK7VrDtvVb1cVceH418Bp4H2p9ymYBdwoqpeqap3gEdYmnm55X+Hx4DPJcmM5ltu3Vmr6odV9fZw+gxL7xI/L2fyuYWlb2L3Ab+f5jDnQxy2VNWp4fh1lgLwPkkuAL4OfHWWg61i3XmXS7IL2Az8YtqDDa4GXl12fnK4rV0zvNHyW8DlM5lulTkG3azL3QF8f6oTrW3deZN8CthWVY9Pe5hN095gFpI8BVzVXDq0/KSqKkn3b7d3At+rqpOz+AY3gXnfu5+twMPA/qr642SnPL8kuQ0YATfOe5bVDN/EvgHcPov9zok4VNXu1a4leSPJ1qo6NXwxnW6WfQb4bJI7gYuAzUl+V1VrvT4xz3lJcgnwOHCoqp6ZxpyreA3Ytuz8muG2bs3JJJuADwO/mc147Rzv6WYlyW6WwnxjVf1hRrN11pv3YuDjwNPDN7GrgIUke6um8Kb0VXVO/wG+Bhwcjg8C96+z/nbggY08L0tPI44Cfz+H+TYBrwDXDXP8FLh+xZq/A/5xOL4FeHROn8szmfWTLD0l2zGv/+Z/yrwr1j/N0oup05ln3p+QGXzCLx++kI4DTwGXDbePgIea9fOOw7rzArcB/w08v+zPDTOc8a+Al4cvqkPDbXcDe4fjPwO+A5wA/gP48zl+Pteb9SngjWWfx4U5//+65rwr1k41Dv74tKTW+fCvFZI+AOMgqWUcJLWMg6SWcZDUMg6SWsZBUut/AMHb9+XQOTcvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL0iMGb6nGFl",
        "outputId": "db4f7d38-a95b-43c8-c8f7-c8e593eff42c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plot_one_tensor(green_like)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOtqQNcCnGFp",
        "outputId": "747547f1-d47f-462c-a938-55416d2212e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plot_one_tensor(blue_like)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQcAAAD4CAYAAADhGCPfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAL0UlEQVR4nO3dX4id9Z3H8fdHQ7oXaus/YtBUXZqbWIptD2F70fWiKcS9SIQuW2WlEYRcuEJL24tA7vRGLf1zobAb3IWsN9YKSwdssZqt9Ka6Tqgr2KJJZYux0dhuEYq0rvS7F/O4O47fmUk9/ybJ+wVhnuc8P8/v6+i855yTYU6qCkla6bx5DyBpYzIOklrGQVLLOEhqGQdJrU3zHmA1yWUF18x7DOksd/Q3VXV5d2XDxmEpDIvzHkI6y+VXq13xaYWklnGQ1DIOklrGQVLLOEhqGQdJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGoZB0kt4yCpZRwktYyDpNZYcUhySZInkhwbPl68xtqLkpxIcv84e0qajXEfORwAjlTVduDIcL6au4GfjLmfpBkZNw57gcPD8WHgpm5Rkk8DW4AfjbmfpBkZNw5bqurkcPwaSwF4jyTnAd8Evr7enSXZn2QxySK8MeZoksax7m+fTvIkcEVz6eDyk6qqJN278t4B/KCqTiRZc6+qOgQcWtp35Dv8SnO0bhyqatdq15K8nmRrVZ1MshU41Sz7DPDZJHcAFwCbk/y+qtZ6fULSnI37vhULwD7gnuHj91cuqKq/f/c4yW3AyDBIG9+4rzncA3w+yTFg13BOklGSB8cdTtL8pGpjPrVfes3Bd7ySpitHq2rUXfEnJCW1jIOklnGQ1DIOklrGQVLLOEhqGQdJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGoZB0kt4yCpZRwktYyDpJZxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1DIOklrGQVLLOEhqGQdJLeMgqWUcJLWMg6TWWHFIckmSJ5IcGz5e3Ky5PslPk7yQ5PkkXxxnT0mzMe4jhwPAkaraDhwZzld6C/hSVV0H7Aa+k+QjY+4racrGjcNe4PBwfBi4aeWCqnqpqo4Nx78GTgGXj7mvpCkbNw5bqurkcPwasGWtxUl2ApuBX465r6Qp27TegiRPAlc0lw4uP6mqSlJr3M9W4CFgX1X9aZU1+4H9S2cfXW80SVO0bhyqatdq15K8nmRrVZ0cvvhPrbLuIuAx4GBVPb3GXoeAQ0v/zGjV0EiavnGfViwA+4bjfcD3Vy5Ishn4N+Bfq+rRMfeTNCPjxuEe4PNJjgG7hnOSjJI8OKz5O+CvgduSPDf8uX7MfSVNWao25qP3pacVi/MeQzrL5WhVjbor/oSkpJZxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1DIOklrGQVLLOEhqGQdJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGoZB0kt4yCpZRwktYyDpJZxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1DIOkloTiUOS3UleTHI8yYHm+oeSfHe4/kySayaxr6TpGTsOSc4HHgBuBHYAtyTZsWLZ7cDvqupjwLeBe8fdV9J0TeKRw07geFW9XFVvAw8De1es2QscHo4fBT6XJBPYW9KUTCIOVwKvLDs/MdzWrqmqd4A3gUtX3lGS/UkWkyzCGxMYTdIHtaFekKyqQ1U1qqoRXD7vcaRz2iTi8Cqwbdn5VcNt7Zokm4APA7+dwN6SpmQScXgW2J7k2iSbgZuBhRVrFoB9w/HfAv9eVTWBvSVNyaZx76Cq3klyJ/A4cD7wL1X1QpK7gMWqWgD+GXgoyXHgv1kKiKQNLBv1G3gyKlic9xjSWS5Hl17je78N9YKkpI3DOEhqGQdJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGoZB0kt4yCpZRwktYyDpJZxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1DIOklrGQVLLOEhqGQdJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGpNJA5Jdid5McnxJAea619N8vMkzyc5kuTqSewraXrGjkOS84EHgBuBHcAtSXasWPYzYFRVnwAeBe4bd19J0zWJRw47geNV9XJVvQ08DOxdvqCqflxVbw2nTwNXTWBfSVM0iThcCbyy7PzEcNtqbgd+OIF9JU3RplluluRWYATcsMr1/cD+pbOPzmwuSe83iUcOrwLblp1fNdz2Hkl2AQeBPVX1x+6OqupQVY2qagSXT2A0SR/UJOLwLLA9ybVJNgM3AwvLFyT5JPBPLIXh1AT2lDRlY8ehqt4B7gQeB34BPFJVLyS5K8meYdk3gAuA7yV5LsnCKncnaYNIVc17hlYyKlic9xjSWS5Hl57Gv58/ISmpZRwktYyDpJZxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1DIOklrGQVLLOEhqGQdJLeMgqWUcJLWMg6SWcZDUMg6SWsZBUss4SGoZB0kt4yCpZRwktYyDpJZxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOk1kTikGR3kheTHE9yYI11X0hSSUaT2FfS9IwdhyTnAw8ANwI7gFuS7GjWXQh8GXhm3D0lTd8kHjnsBI5X1ctV9TbwMLC3WXc3cC/whwnsKWnKJhGHK4FXlp2fGG77P0k+BWyrqsfWuqMk+5MsJlmENyYwmqQPauovSCY5D/gW8LX11lbVoaoaVdUILp/2aJLWMIk4vApsW3Z+1XDbuy4EPg48leS/gL8CFnxRUtrYJhGHZ4HtSa5Nshm4GVh492JVvVlVl1XVNVV1DfA0sKeqFiewt6QpGTsOVfUOcCfwOPAL4JGqeiHJXUn2jHv/kuYjVTXvGVrJqMAHF9J05ejSa3zv509ISmoZB0kt4yCpZRwktYyDpJZxkNQyDpJaxkFSyzhIahkHSS3jIKllHCS1jIOklnGQ1DIOklrGQVJrA/+yl7wB/GoKd30Z8Jsp3O+0nEnznkmzwpk177Rmvbqq2t/mvGHjMC1JFlf7zTcb0Zk075k0K5xZ885jVp9WSGoZB0mtczEOh+Y9wJ/pTJr3TJoVzqx5Zz7rOfeag6TTcy4+cpB0GoyDpNZZH4cklyR5Ismx4ePFa6y9KMmJJPfPcsYVM6w7b5Lrk/w0yQtJnk/yxRnPuDvJi0mOJznQXP9Qku8O159Jcs0s51sxy3qzfjXJz4fP45EkV89jzmXzrDnvsnVfSFLTfM/Zsz4OwAHgSFVtB44M56u5G/jJTKZa3enM+xbwpaq6DtgNfCfJR2YxXJLzgQeAG4EdwC1JdqxYdjvwu6r6GPBt4N5ZzLbSac76M2BUVZ8AHgXum+2U/+805yXJhcCXgWemOc+5EIe9wOHh+DBwU7coyaeBLcCPZjTXatadt6peqqpjw/GvgVNA+1NuU7ATOF5VL1fV28DDLM283PJ/h0eBzyXJjOZbbt1Zq+rHVfXWcPo0S+8SPy+n87mFpW9i9wJ/mOYw50IctlTVyeH4NZYC8B5JzgO+CXx9loOtYt15l0uyE9gM/HLagw2uBF5Zdn5iuK1dM7zR8pvApTOZbpU5Bt2sy90O/HCqE61t3XmTfArYVlWPTXuYTdPeYBaSPAlc0Vw6uPykqipJ93e3dwA/qKoTs/gGN4F5372frcBDwL6q+tNkpzy3JLkVGAE3zHuW1QzfxL4F3DaL/c6KOFTVrtWuJXk9ydaqOjl8MZ1qln0G+GySO4ALgM1Jfl9Va70+Mc95SXIR8BhwsKqensacq3gV2Lbs/Krhtm7NiSSbgA8Dv53NeO0c7+pmJckulsJ8Q1X9cUazddab90Lg48BTwzexK4CFJHuqavJvSV9VZ/Uf4BvAgeH4AHDfOutvA+7fyPOy9DTiCPCVOcy3CXgZuHaY4z+B61as+QfgH4fjm4FH5vS5PJ1ZP8nSU7Lt8/pv/ufMu2L9Uyy9mDqdeeb9CZnBJ/zS4QvpGPAkcMlw+wh4sFk/7zisOy9wK/A/wHPL/lw/wxn/Bnhp+KI6ONx2F7BnOP4L4HvAceA/gL+c4+dzvVmfBF5f9nlcmPP/r2vOu2LtVOPgj09Lap0Lf1sh6QMwDpJaxkFSyzhIahkHSS3jIKllHCS1/hesvfflERhdIwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7Ucg7xRQDwn"
      },
      "source": [
        "## What Pytorch can do\n",
        "\n",
        "*   Similar functions to Numpy on GPU\n",
        "*   Calculate automatically gradient on the neural network\n",
        "*   Some neural networks layers are already coded : dense, convolution, pooling, etc\n",
        "*   Calculate automatically the weights update\n",
        "*   Provide optimizer to compute gradient descent \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjeKBFJHXYl8",
        "outputId": "5f85d3df-4707-45f8-f533-3eeb5f146507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mat_torch = torch.arange(15).reshape(3,5)\n",
        "\n",
        "print(mat_torch) # Create a vector from 0 to 14 and reshape it into a Matrix 3X5\n",
        "print(mat_torch.shape) # Return the size of the matrix (3, 5)\n",
        "print(mat_torch[0]) # Return the first row of the matrix \n",
        "print(mat_torch[0,3]) # Return first row and 4th column element \n",
        "# This was easy but everything was on the CPU so it's the same as Numpy \n",
        "# To do computation on the GPU (graphic card calculation can be 50x faster)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0,  1,  2,  3,  4],\n",
            "        [ 5,  6,  7,  8,  9],\n",
            "        [10, 11, 12, 13, 14]])\n",
            "\n",
            "torch.Size([3, 5])\n",
            "\n",
            "tensor([0, 1, 2, 3, 4])\n",
            "\n",
            "tensor(3)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4fBZlMHYMqI",
        "outputId": "56fef755-b0d9-4e18-af45-a8931dcde972",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# What is the GPU on this machine ? \n",
        "#!nvidia-smi\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujP5e7pGYLCh",
        "outputId": "f0f78178-a691-4c2a-f9cb-ab303568f9a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mat_torch = torch.arange(15, device=device).reshape(3,5)\n",
        "print(int(mat_torch.max())) # Create a vector from 0 to 14 and reshape it into a Matrix 3X5\n",
        "print(mat_torch.shape) # Return the size of the matrix (3, 5)\n",
        "print(mat_torch[0]) # Return the first row of the matrix \n",
        "print(mat_torch[0,3]) # Return first row and 4th column element "
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n",
            "\n",
            "torch.Size([3, 5])\n",
            "\n",
            "tensor([0, 1, 2, 3, 4], device='cuda:0')\n",
            "\n",
            "tensor(3, device='cuda:0')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YQO8LsGZ8iq"
      },
      "source": [
        "Let's say we want a faster sigmoid and softmax. \n",
        "We can use the same function from TP-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0bUELpAYn0O"
      },
      "source": [
        "def normalize_tensor(input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Apply a normalization to the tensor\"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    return input_tensor/255\n",
        "   \n",
        "def sigmoid(input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Apply a sigmoid to the input Tensor\"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    return 1/(1+torch.exp(-input_tensor))\n",
        "\n",
        "def softmax(input_tensor: torch.Tensor)-> torch.Tensor:\n",
        "    \"\"\"Apply a softmax to the input tensor\"\"\"\n",
        "    # YOUR CODE HERE \n",
        "    exp = torch.exp(input_tensor)\n",
        "    sum_exp = torch.sum(exp,axis=1).reshape(-1,1)\n",
        "    return exp/sum_exp\n",
        "\n",
        "def target_to_one_hot(target: torch.Tensor, num_classes=10) -> torch.Tensor:\n",
        "    \"\"\"Create the one hot representation of the target\"\"\" \n",
        "    # YOUR CODE HERE \n",
        "    one_hot_matrix = torch.zeros((target.shape[0],num_classes)) \n",
        "    for i in range(target.shape[0]):\n",
        "      one_hot_matrix[i][target[i]] = 1\n",
        "    return one_hot_matrix\n",
        "\n"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Je20wNZuj0"
      },
      "source": [
        "# However as mention above pytorch already has some built-ins function \n",
        "\n",
        "# sigmoid function [sigmoid doc](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html?highlight=sigmoid#torch.nn.Sigmoid)\n",
        "# softmax function [softmax doc](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html?highlight=softmax#torch.nn.Softmax) \n"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXpmI-kTbq4T"
      },
      "source": [
        "mat_torch = torch.arange(15, dtype=torch.float64, device=device).reshape(3,5)\n",
        "# Uncomment the line bellow to check if your implementation is correct\n",
        "\n",
        "assert torch.allclose(sigmoid(mat_torch), torch.sigmoid(mat_torch))\n",
        "print(sigmoid(mat_torch))\n",
        "print(torch.sigmoid(mat_torch))\n",
        "\n",
        "assert torch.allclose(softmax(mat_torch), torch.softmax(mat_torch, dim=1))\n",
        "print(softmax(mat_torch))\n",
        "print(torch.softmax(mat_torch, dim=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp405FXquGqz"
      },
      "source": [
        "## Transforming our Neural network from TP1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCnZAR7x2yUl"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Downloading again the same MNIST dataset \n",
        "\n",
        "    mnist_data, mnist_target = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(mnist_data, mnist_target, test_size=0.33, random_state=1342)\n",
        "    # Change the input data to be normalize and target data to be correctly encoded \n",
        "\n",
        "    X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "    X_train = normalize_tensor(X_train)\n",
        "\n",
        "    X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "    X_test = normalize_tensor(X_test)\n",
        "\n",
        "    y_train = torch.from_numpy(y_train.astype(np.int32))\n",
        "    y_train = target_to_one_hot(y_train)\n",
        "\n",
        "    y_test = torch.from_numpy(y_test.astype(np.int32))\n",
        "    y_test = target_to_one_hot(y_test)\n"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYxuYKhUuT6U"
      },
      "source": [
        "Your remember the famous `class FFNN` from **TP1** ?? \n",
        "\n",
        "Here we will create the same version but with pytorch and we will see the power of this framework. \n",
        "\n",
        "Auto calculation of the backward pass and auto update of the weights 🎉 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x677wA4zvtMR"
      },
      "source": [
        "In pytorch a dense layer similar to our `Class Layer` is a called **Linear Layer**\n",
        "\n",
        "[linear layer documentation] -> https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlBuCmXNxIRY"
      },
      "source": [
        "class FFNN(nn.Module):\n",
        "    def __init__(self, config, device, minibatch_size=100, learning_rate=0.01, momentum=0):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        self.nlayers = len(config)\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.device = device \n",
        "\n",
        "        # We use the built-in activation functions\n",
        "        # TODO: Maybe try with another activation function ! \n",
        "        #self.activation = torch.nn.Sigmoid()\n",
        "        self.activation = torch.nn.ReLU()\n",
        "\n",
        "\n",
        "        self.last_activation = torch.nn.Softmax(dim=1)\n",
        "\n",
        "        # First difference we don't need a special Input layer 😃\n",
        "        # Second one we can declare them more easely\n",
        "        for i in range(1,len(config)):\n",
        "          layer = nn.Linear(config[i-1], config[i])\n",
        "          self.layers.append(layer)\n",
        "          self.layers.append(self.activation)\n",
        "\n",
        "        self.layers[-1]= self.last_activation\n",
        "        self.model = nn.Sequential(*self.layers)\n",
        "\n",
        "        # We use the built-in function to compute the loss\n",
        "        # TODO: Maybe try with another loss function ! \n",
        "        #self.loss_function = torch.nn.MSELoss()\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # We use the built-in function to update the model weights\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum=self.momentum)\n",
        "\n",
        "    # Here we see the power of Pytorch\n",
        "    # The forward is just giving the input to our model\n",
        "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "      y_pred = self.model(input_tensor)\n",
        "      return y_pred\n",
        "\n",
        "    def compute_loss(self, y_pred: torch.Tensor, y_true) -> torch.Tensor:\n",
        "        y_true = torch.argmax(y_true, dim=1)\n",
        "        loss = self.loss_function(y_pred.float(), y_true)\n",
        "        # looking at what the loss looks like\n",
        "        #print(loss)\n",
        "        return loss\n",
        "\n",
        "    # Even more powerful no need to code all the derivative of the different function\n",
        "    def backward_pass(self, loss: torch.tensor) -> None:\n",
        "        loss.backward()\n",
        "        return\n",
        "\n",
        "    # The previoulsy hard function to update the weight become also easy\n",
        "    def update_all_weights(self):\n",
        "      # Using pytorch\n",
        "      self.optimizer.step()\n",
        "\n",
        "\n",
        "    def get_error(self, y_pred, y_true) -> float:\n",
        "      y_pred = torch.argmax(y_pred, dim=1)\n",
        "      y_true = torch.argmax(y_true, dim=1)\n",
        "      return (y_pred == y_true).float().mean()\n",
        "\n",
        "    def get_test_error(self, X_test, y_test) -> float:\n",
        "      nbatch = X_test.shape[0]\n",
        "      error_sum = 0.0\n",
        "      for i in range(0, nbatch):\n",
        "          X_batch = X_test[i,:,:].reshape(self.minibatch_size, -1)\n",
        "          y_batch = y_test[i,:,:].reshape(self.minibatch_size, -1)\n",
        "          y_pred = self.model(X_batch)\n",
        "          error_sum += self.get_error(y_pred, y_batch)\n",
        "      return error_sum / nbatch\n",
        "\n",
        "    def train(self, n_epochs: int, X_train: torch.Tensor, y_train: torch.Tensor, X_test: torch.Tensor, y_test: torch.Tensor):\n",
        "      X_train = X_train.reshape(-1, self.minibatch_size, 784).to(self.device)\n",
        "      y_train = y_train.reshape(-1, self.minibatch_size, 10).to(self.device)\n",
        "\n",
        "      X_test = X_test.reshape(-1, self.minibatch_size, 784).to(self.device)\n",
        "      y_test = y_test.reshape(-1, self.minibatch_size, 10).to(self.device)\n",
        "\n",
        "      \n",
        "      self.model = self.model.to(device)\n",
        "      nbatch = X_train.shape[0]\n",
        "      error_test = 0.0\n",
        "      for epoch in range(n_epochs): \n",
        "        error_sum_train = 0.0\n",
        "        for i in range(0, nbatch):\n",
        "          X_batch = X_train[i,:, :]\n",
        "          y_batch = y_train[i,:, :]\n",
        "          # In order to have the correct derivative we remove the one from before \n",
        "          self.optimizer.zero_grad()\n",
        "          # Then we do a pass forward \n",
        "          y_pred = self.model(X_batch)\n",
        "          # We compute the loss \n",
        "          loss = self.compute_loss(y_pred, y_batch)\n",
        "          # And calculate the backward pass\n",
        "          self.backward_pass(loss=loss)\n",
        "          # To finally update the weights using stochastic gradient descent \n",
        "          self.update_all_weights()\n",
        "          error_sum_train += self.get_error(y_pred, y_batch)\n",
        "        error_test = self.get_test_error(X_test, y_test)\n",
        "        \n",
        "        print(f\"Training Loss: {loss:.3f}, Training accuracy: {error_sum_train / nbatch:.3f}, Test accuracy: {error_test:.3f}\")\n",
        "      return loss, error_test"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAzW9AJMzyOq",
        "tags": []
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    minibatch_size = 28\n",
        "    nepoch = 50\n",
        "    learning_rate = 0.1\n",
        "    ffnn = FFNN(config=[784, 256, 128, 10], device=device, minibatch_size=minibatch_size, learning_rate=learning_rate)\n",
        "    print(ffnn)\n",
        "    loss, err = ffnn.train(nepoch, X_train, y_train, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTQ_8e8gK3RA"
      },
      "source": [
        "In pytorch a very convinient way to load data in batch is to use the data loader. \n",
        "\n",
        "Let's update the class to use it, we are also going to use dataset available in pytorch vision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei5R8mqlxOZi"
      },
      "source": [
        "class FFNNModel(nn.Module):\n",
        "    def __init__(self, classes=10):\n",
        "        super().__init__()\n",
        "        # not the best model...\n",
        "        self.l1 = torch.nn.Linear(784, 256)\n",
        "        self.l2 = torch.nn.Linear(256, 128)\n",
        "        self.l3 = torch.nn.Linear(128, classes)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        self.last_activation = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = input.reshape(input.size(0), -1)\n",
        "        x = self.l1(input)\n",
        "        x = self.activation(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.l3(x)\n",
        "        y = self.last_activation(x)\n",
        "        return y\n",
        "\n",
        "def train_one_epoch(model, device, data_loader, optimizer):\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    for num, (data, target) in tq.tqdm(enumerate(data_loader), total=len(data_loader.dataset)/data_loader.batch_size):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        prediction = output.argmax(dim=1)\n",
        "        correct += torch.sum(prediction.eq(target)).item()\n",
        "\n",
        "    result = {'loss': train_loss / len(data_loader.dataset),\n",
        "              'accuracy': correct / len(data_loader.dataset)\n",
        "              }\n",
        "    return result   \n",
        " \n",
        "def evaluation(model, device, data_loader):\n",
        "    eval_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    for num, (data, target) in tq.tqdm(enumerate(data_loader), total=len(data_loader.dataset)/data_loader.batch_size):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        eval_loss += F.cross_entropy(output, target).item()\n",
        "        prediction = output.argmax(dim=1)\n",
        "        correct += torch.sum(prediction.eq(target)).item()\n",
        "    result = {'loss': eval_loss / len(data_loader.dataset),\n",
        "              'accuracy': correct / len(data_loader.dataset)\n",
        "              }\n",
        "    return result"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hcj3lBVPgeIN"
      },
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    # Network Hyperparameters \n",
        "    minibatch_size = 28\n",
        "    nepoch = 10\n",
        "    learning_rate = 0.1\n",
        "    momentum = 0 \n",
        "    model = FFNNModel()\n",
        "    model.to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "    # Retrieve the data with the pytorch dataloader \n",
        "    mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\n",
        "    mnist_train = DataLoader(mnist_train, batch_size=32, num_workers=4, pin_memory=True)\n",
        "    mnist_val = MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor())\n",
        "    mnist_val = DataLoader(mnist_val, batch_size=32, num_workers=4,  pin_memory=True)\n",
        "\n",
        "    # Train for an number of epoch \n",
        "    for epoch in range(nepoch):\n",
        "      print(f\"training Epoch: {epoch}\")\n",
        "      if epoch > 0:\n",
        "        train_result = train_one_epoch(model, device, mnist_train, optimizer)\n",
        "        print(f\"Result Training dataset {train_result}\")\n",
        "\n",
        "      eval_result = evaluation(model, device, mnist_val)\n",
        "      print(f\"Result Test dataset {eval_result}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR0RUMI0WABc"
      },
      "source": [
        "# Part 1: What is a convolution ?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q06D0V9KnzJv"
      },
      "source": [
        "In this section you will implement 2D convolution operation using:\n",
        "\n",
        "Starting with a simple example and manual computation like in Lecture 2\n",
        "\n",
        "1) Introduction: manual computation\n",
        "\n",
        "- you have as input an image of 5x5 pixels\n",
        "\n",
        "$I = \\begin{bmatrix}I_{1, 1} & ... & I_{1, 5} \\\\ \\vdots & \\ddots & \\vdots \\\\ I_{5, 1}& ... & I_{5,5}\\end{bmatrix}$\n",
        "\n",
        "Your task is to compute the result of a convolution operation between this image and a 3x3 kernel\n",
        "\n",
        "$ K = \\begin{bmatrix}a & b & c \\\\d & e & f \\\\ g& h& i\\end{bmatrix}$\n",
        "\n",
        "We are considering padding with 0 and using the SAME convolution. \n",
        "Meaning that arround the I matrix consider there is the value 0.\n",
        "\n",
        "Tips: the result of the convolution is a 5x5 matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfvn5c9yWABe"
      },
      "source": [
        "I = np.array([[252,  49, 113,  11, 137],\n",
        "                [ 18, 237, 163, 119,  53],\n",
        "                [ 90,  89, 178,  75, 247],\n",
        "                [209, 216,  48, 135, 232],\n",
        "                [229, 53, 107, 106, 222]])\n",
        "print(f\"I =\")\n",
        "print(I.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDhH5cxzWABl",
        "outputId": "54d8b5fe-1f01-4e16-fd13-716c40467709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "K_0 = np.array([[0, 1, 0], [0, 0, 0], [0, 0, 0]])\n",
        "#print(f\"K_0 =\")\n",
        "print(K_0)\n",
        "\n",
        "K_1 = np.array([[1, 1, 1], [0, 5, 0], [-1, -1, -1]])\n",
        "print(f\"K_1 =\")\n",
        "print(K_1)\n"
      ],
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "\n",
            "K_1 =\n",
            "\n",
            "[[ 1  1  1]\n",
            " [ 0  5  0]\n",
            " [-1 -1 -1]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqaSC3rTWABq"
      },
      "source": [
        "What is the result of convolution of $ I_0 \\ast K_0 $\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WidbhmkRWABr"
      },
      "source": [
        "# put your answer here\n",
        "R_0 = np.array([[0,0,0,0,0],\n",
        "               [252,49,113,11,137],\n",
        "               [18,237,163,119,53],\n",
        "               [90,89,178,75,247],\n",
        "               [209,216,48,135,232]]).astype(np.float32)"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfi2u2yVWABw"
      },
      "source": [
        "What is the result of convolution of $ I_0 \\ast K_1 $\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XKt_u-wWABx",
        "outputId": "4338df5d-7b5f-43ba-f829-1d029a0474fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# put your answer here\n",
        "R_1 = np.array([[1005,-173,46,-280,513],\n",
        "               [212,1242,646,356,91],\n",
        "               [280,390,1010,295,1040],\n",
        "               [942,1048,316,740,1154],\n",
        "               [1570,738,934,945,1477]]).astype(np.float32)\n",
        "\n",
        "R_1"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1005., -173.,   46., -280.,  513.],\n",
              "       [ 212., 1242.,  646.,  356.,   91.],\n",
              "       [ 280.,  390., 1010.,  295., 1040.],\n",
              "       [ 942., 1048.,  316.,  740., 1154.],\n",
              "       [1570.,  738.,  934.,  945., 1477.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu-2xPRZWAB0"
      },
      "source": [
        "## 2) Computation using __numpy__\n",
        "\n",
        "Now using the numpy implement the convolution operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP7fnMBHetJg"
      },
      "source": [
        "def convolution_forward_numpy(image, kernel):\n",
        "    # YOUR CODE HERE \n",
        "\n",
        "    RGB = False\n",
        "    image_pad = np.zeros((image.shape[0]+kernel.shape[0]-1,image.shape[1]+kernel.shape[0]-1))\n",
        "    image_pad_red = np.zeros((image.shape[0]+kernel.shape[0]-1,image.shape[1]+kernel.shape[0]-1),dtype=int)\n",
        "    image_pad_green = np.zeros((image.shape[0]+kernel.shape[0]-1,image.shape[1]+kernel.shape[0]-1),dtype=int)\n",
        "    image_pad_blue = np.zeros((image.shape[0]+kernel.shape[0]-1,image.shape[1]+kernel.shape[0]-1),dtype=int)\n",
        "\n",
        "    convolution_matrix = np.zeros(image.shape,dtype=int)\n",
        "    conv_mat_red = np.zeros((image.shape[0],image.shape[1]),dtype=int)\n",
        "    conv_mat_green = np.zeros((image.shape[0],image.shape[1]),dtype=int)\n",
        "    conv_mat_blue = np.zeros((image.shape[0],image.shape[1]),dtype=int)\n",
        "\n",
        "    if len(image.shape) == 3:\n",
        "        RGB = True\n",
        "\n",
        "    for i in range(len(image)):\n",
        "        for j in range(len(image[i])):\n",
        "            if RGB:\n",
        "                  image_pad_red[i+1][j+1] = image[i][j][0]\n",
        "                  image_pad_green[i+1][j+1] = image[i][j][1]\n",
        "                  image_pad_blue[i+1][j+1] = image[i][j][2]\n",
        "            else:\n",
        "                image_pad[i+1][j+1] = image[i][j]\n",
        "    \n",
        "\n",
        "\n",
        "    for i in range(len(image)):\n",
        "        for j in range(len(image[i])):\n",
        "            for k in range(len(kernel)):\n",
        "                for l in range(len(kernel[k])):\n",
        "                    if RGB:\n",
        "                        conv_mat_red[i][j] += image_pad_red[k+i][l+j]*kernel[k][l]\n",
        "                        conv_mat_green[i][j] += image_pad_green[k+i][l+j]*kernel[k][l]\n",
        "                        conv_mat_blue[i][j] += image_pad_blue[k+i][l+j]*kernel[k][l]\n",
        "                    else:\n",
        "                        convolution_matrix[i][j] += image_pad[k+i][l+j]*kernel[k][l]\n",
        "\n",
        "    if RGB:\n",
        "        for i in range(len(image)):\n",
        "            for j in range(len(image[i])):\n",
        "                convolution_matrix[i][j] = [conv_mat_red[i][j],conv_mat_green[i][j],conv_mat_blue[i][j]]\n",
        "\n",
        "    return convolution_matrix\n",
        "  \n",
        "test = np.array([[[0,255,0],[0,255,0],[0,255,0],[50,40,40],[60,60,55]],\n",
        "                   [[10,95,0],[6,25,0],[0,55,0],[70,60,40],[80,60,55]],\n",
        "                   [[70,255,0],[0,25,80],[0,25,0],[50,40,43],[65,60,55]],\n",
        "                   [[0,255,0],[1,255,0],[3,255,0],[50,80,40],[64,60,55]],\n",
        "                   [[200,150,120],[9,10,20],[40,50,60],[70,40,6],[4,250,3]]])\n",
        "\n",
        "print(convolution_forward_numpy(test,K_0),\"AAAAAAAAAAAAAAAAAAAAAAAAAAAA\",convolution_forward_torch(test,K_0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OGXKtyVetJm"
      },
      "source": [
        "Test your implementation on the two previous example and compare the results to the result manually computed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRubH1y6WAB1",
        "lines_to_next_cell": 2
      },
      "source": [
        "\n",
        "#assert convolution_forward_numpy(I, K_0) == R_0\n",
        "#assert convolution_forward_numpy(I, K_1) == R_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Lmj8tf3WACI"
      },
      "source": [
        "Display the result image of the convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7z9q-rtWACQ"
      },
      "source": [
        "## 3) Computation using __pytorch__\n",
        "\n",
        "Now let's use pytorch convolution layer to do the forward pass. Use the documentation available at: https://pytorch.org/docs/stable/nn.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoEYVPzFWACR"
      },
      "source": [
        "# Load image from url, you can use an other image if you want\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/4/4f/ECE_Paris_Lyon.jpg\"\n",
        "image = imageio.imread(image_url)\n",
        "\n",
        "\n",
        "# simple function to display image\n",
        "def display_image(img):\n",
        "    plt.imshow(img)\n",
        "\n",
        "# display the image\n",
        "#display_image(image)\n",
        "# Do the convolution operation and display the resulting image\n",
        "image = np.array(image)\n",
        "# YOUR CODE HERE\n",
        "#output_image = convolution_forward_numpy(image, K_1) \n",
        "#display_image(output_image)"
      ],
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZboYqQCWACW",
        "lines_to_next_cell": 2,
        "outputId": "00db0d2d-1467-4eec-8166-9ee243de3c0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        }
      },
      "source": [
        "def convolution_forward_torch(image, kernel):\n",
        "    # YOUR CODE HERE \n",
        "\n",
        "    image = torch.Tensor(image.reshape(1,3,image.shape[0],image.shape[1]))\n",
        "    print(image)\n",
        "    kernel = torch.Tensor(kernel) #reshape(1,1,3,3)\n",
        "    kernel = kernel.view(1, 1, 3, 3).repeat(1, 3, 1, 1)\n",
        "    print(kernel)\n",
        "    convolution = torch.nn.functional.conv2d(image, kernel, padding=1, stride=1)\n",
        "    return convolution\n",
        "\n",
        "output_image_torch = convolution_forward_torch(image, K_0)\n",
        "print(output_image_torch.shape)\n",
        "output_image_torch = torch.reshape(output_image_torch,(output_image_torch.shape[2],output_image_torch.shape[3]))\n",
        "output_image = output_image_torch.numpy()[:,:]\n",
        "display_image(output_image_torch)"
      ],
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[  0., 113., 121.,  ...,   0., 113., 121.],\n",
            "          [  0., 113., 121.,  ...,   0., 113., 121.],\n",
            "          [  0., 113., 121.,  ...,   0., 113., 121.],\n",
            "          ...,\n",
            "          [  0., 113., 121.,  ...,   1., 112., 123.],\n",
            "          [  1., 112., 123.,  ...,   0., 113., 121.],\n",
            "          [  0., 113., 121.,  ...,   0., 113., 121.]],\n",
            "\n",
            "         [[  0., 113., 121.,  ...,   1., 112., 123.],\n",
            "          [  1., 112., 123.,  ...,   0., 113., 121.],\n",
            "          [  0., 113., 121.,  ...,   0., 113., 121.],\n",
            "          ...,\n",
            "          [  0., 113., 121.,  ...,  13., 116., 125.],\n",
            "          [ 26., 120., 128.,  ...,  26.,  90.,  92.],\n",
            "          [141., 189., 191.,  ...,   0., 113., 121.]],\n",
            "\n",
            "         [[  0., 113., 121.,  ...,   0.,  97., 104.],\n",
            "          [ 36., 125., 131.,  ...,  26., 101., 106.],\n",
            "          [ 41., 100., 104.,  ...,   0., 113., 121.],\n",
            "          ...,\n",
            "          [  0., 113., 121.,  ...,   0., 113., 121.],\n",
            "          [  0., 113., 121.,  ...,   0., 113., 121.],\n",
            "          [  0., 113., 121.,  ...,   0., 113., 121.]]]])\n",
            "\n",
            "tensor([[[[0., 1., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 1., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 1., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]]])\n",
            "\n",
            "torch.Size([1, 1, 1638, 1638])\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9XtW00JWACZ"
      },
      "source": [
        "In pytorch you can also access other layer like convolution2D, pooling layers, for example in the following cell use the __torch.nn.MaxPool2d__ to redduce the image size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEpb5XVyWACf",
        "lines_to_next_cell": 2
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5aVluRsoVMC"
      },
      "source": [
        "# Part 2: Using convolution neural network to recognize digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YkGWHXIWACi"
      },
      "source": [
        "In this section you will implement 2D convolution neural network and train it on fashion mnist dataset\n",
        "\n",
        "https://github.com/zalandoresearch/fashion-mnist\n",
        "\n",
        "\n",
        "![Image of fashion mnist](https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/doc/img/fashion-mnist-sprite.png)\n",
        "\n",
        "##  First let's look at the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfBxOSnDIwle"
      },
      "source": [
        "if __name__ == \"__main__\" :\n",
        "\n",
        "  fmnist_train = FashionMNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\n",
        "  fmnist_train = DataLoader(fmnist_train, batch_size=32, num_workers=4, pin_memory=True)\n",
        "  fmnist_val = FashionMNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor())\n",
        "  fmnist_val = DataLoader(fmnist_val, batch_size=32, num_workers=4,  pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWhl5or3WACl"
      },
      "source": [
        "Display the 10 image from train set and 10 images from validation set, print their ground truth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOKRW0eOEt4G"
      },
      "source": [
        "def display_10_images(dataset):\n",
        "    # YOUR CODE HERE \n",
        "    NotImplemented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNl2tW4OWACm"
      },
      "source": [
        "What is the shape of each images\n",
        "How many images do we have\n",
        "What are the different classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ex3UohlH0o6"
      },
      "source": [
        "def fashion_mnist_dataset_answer():\n",
        "    shape = None  # replace None with the value you found\n",
        "    number_of_images_in_train_set = None\n",
        "    number_of_images_in_test_set = None\n",
        "    number_of_classes = None\n",
        "    return {'shape': shape, 'nb_in_train_set': number_of_images_in_train_set, 'nb_in_test_set': number_of_images_in_test_set, 'number_of_classes': number_of_classes}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCt97SpbI7pO"
      },
      "source": [
        "# Plot an image and the target  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHK65RunWADA"
      },
      "source": [
        "## Create a convolutional neural network\n",
        "\n",
        "Now it's your turn to create a convolutional neural network and to train your model on the fashion mnist dataset.\n",
        "\n",
        "Classical machine learning approach manage to get a 89% accuracy on fashion mnist, your objective is to use deep learning (and convolution neural network) to get more than 90%\n",
        "\n",
        "You can first start with this simple convolution network and improve it by adding/modifying the layers used:\n",
        "\n",
        "```\n",
        "convolutional layer 3x3\n",
        "convolutional layer 3x3\n",
        "max-pooling\n",
        "convolutional layer 3x3\n",
        "convolutional layer 3x3\n",
        "max-pooling\n",
        "flatten\n",
        "fully-connected layer (dense layer)\n",
        "fully-connected layer (dense layer)\n",
        "fully-connected layer (dense layer)\n",
        "Softmax\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W7t-is0WADA",
        "lines_to_next_cell": 2
      },
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, classes=10):\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE \n",
        "        self.conv1 = NotImplemented\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        # YOUR CODE HERE \n",
        "        y = NotImplemented\n",
        "        return y\n",
        "\n",
        "def train_one_epoch(model, device, data_loader, optimizer):\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    for num, (data, target) in tq.tqdm(enumerate(data_loader), total=len(data_loader.dataset)/data_loader.batch_size):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "\n",
        "        # YOUR CODE HERE \n",
        "        loss = NotImplemented\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        prediction = output.argmax(dim=1)\n",
        "        correct += torch.sum(prediction.eq(target)).item()\n",
        "\n",
        "    result = {'loss': train_loss / len(data_loader.dataset),\n",
        "              'accuracy': correct / len(data_loader.dataset)\n",
        "              }\n",
        "    return result   \n",
        " \n",
        "def evaluation(model, device, data_loader):\n",
        "    eval_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    for num, (data, target) in tq.tqdm(enumerate(data_loader), total=len(data_loader.dataset)/data_loader.batch_size):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        # YOUR CODE HERE \n",
        "        eval_loss = NotImplemented\n",
        "        prediction = output.argmax(dim=1)\n",
        "        correct += torch.sum(prediction.eq(target)).item()\n",
        "    result = {'loss': eval_loss / len(data_loader.dataset),\n",
        "              'accuracy': correct / len(data_loader.dataset)\n",
        "              }\n",
        "    return result\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    # Network Hyperparameters \n",
        "    # YOUR CODE HERE \n",
        "    minibatch_size = NotImplemented\n",
        "    nepoch = NotImplemented\n",
        "    learning_rate = NotImplemented\n",
        "    momentum = NotImplemented\n",
        "\n",
        "\n",
        "    model = CNNModel()\n",
        "    model.to(device)\n",
        "\n",
        "    # YOUR CODE HERE \n",
        "    optimizer = NotImplemented\n",
        "\n",
        "    # Train for an number of epoch \n",
        "    for epoch in range(nepoch):\n",
        "      print(f\"training Epoch: {epoch}\")\n",
        "      if epoch > 0:\n",
        "        train_result = train_one_epoch(model, device, fmnist_train, optimizer)\n",
        "        print(f\"Result Training dataset {train_result}\")\n",
        "\n",
        "      eval_result = evaluation(model, device, fmnist_val)\n",
        "      print(f\"Result Test dataset {eval_result}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7mcHag9Et4u"
      },
      "source": [
        "## Open Analysis\n",
        "Same as TP 1 please write a short description of your experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCHr8iCAEt4x"
      },
      "source": [
        "# BONUS \n",
        "\n",
        "Use some already trained CNN to segment YOUR image. \n",
        "\n",
        "In the cell below your can load a image to the notebook and use the given network to have the segmentation mask and plot it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBh3R2IGEt4y"
      },
      "source": [
        "if __name__ == \"__main__\" :\n",
        "    \n",
        "    # TODO HERE: Upload an image to the notebook in the navigation bar on the left\n",
        "    # `File` `Load File`and load an image to the notebook. \n",
        "    \n",
        "    filename = \"\" \n",
        "    # Loading a already trained network in pytorch \n",
        "    model = torch.hub.load('pytorch/vision:v0.6.0', 'deeplabv3_resnet101', pretrained=True)\n",
        "    model.eval()\n",
        "\n",
        "    from PIL import Image\n",
        "    from torchvision import transforms\n",
        "\n",
        "    input_image = Image.open(filename)\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    input_tensor = preprocess(input_image)\n",
        "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "    # move the input and model to GPU for speed if available\n",
        "    if torch.cuda.is_available():\n",
        "        input_batch = input_batch.to('cuda')\n",
        "        model.to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_batch)['out'][0]\n",
        "    output_predictions = output.argmax(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_tjfTATEt45"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}